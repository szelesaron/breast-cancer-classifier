{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's start over after we've understood some maths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Data Setup:\n",
    "X = np.load('data/X-data.npy')\n",
    "y = np.load('data/y-data.npy')\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train(X, y, rate=0.2):\n",
    "    # First shuffle randomly \n",
    "    assert len(X) == len(y)\n",
    "    p = np.random.permutation(len(X))\n",
    "    X_shuffled, y_shuffled = X[p], y[p]\n",
    "    \n",
    "    # Split into test and train set\n",
    "    i_test = round(len(X) * 0.2)\n",
    "    X_test, X_train = X_shuffled[:i_test], X_shuffled[i_test:]\n",
    "    y_test, y_train = y_shuffled[:i_test], y_shuffled[i_test:]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "    \n",
    "X_train, y_train, X_test, y_test = split_test_train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract Class Definitions: \n",
    "class Layer():\n",
    "    def __init__(self): \n",
    "        self.inputs = None  # The inputs into this layer. \n",
    "        self.outputs = None # The ouputs of this layer. \n",
    "        \n",
    "    # Forward propagation method.\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "    \n",
    "    # Backward propagation method.\n",
    "    def backward(self):\n",
    "        pass\n",
    "    \n",
    "class Activation():\n",
    "    def activate():\n",
    "        pass\n",
    "    \n",
    "    def prime():\n",
    "        pass\n",
    "    \n",
    "class Loss():\n",
    "    def calculate_loss():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    def activate(self, x): \n",
    "        return np.maximum(x, 0.0)\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        return (x > 0) * 1  # * 1 to return a number.\n",
    "    \n",
    "class Sigmoid(Activation):\n",
    "    def activate(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def derivative(self, x):\n",
    "        s = self.activate(x)\n",
    "        return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss():\n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    def derivative(self, y_true, y_pred): \n",
    "        r = ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)\n",
    "        return r\n",
    "    \n",
    "class MSELoss():\n",
    "    def calculate_loss(self, y_true, y_pred):\n",
    "        return 1/2 * (y_true - y_pred) * (y_true - y_pred)\n",
    "    \n",
    "    def derivative(self, y_true, y_pred):\n",
    "        return y_true - y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense (fully connected) Layer Class:\n",
    "class Dense(Layer): \n",
    "    def __init__(self, input_size, output_size, activation_function='relu', name='unnamed'):\n",
    "        self.name = name\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.biases = np.zeros((output_size, 1))\n",
    "        self.outputs = None\n",
    "        \n",
    "        if activation_function == 'relu':\n",
    "            self.activation = ReLU()\n",
    "        elif activation_function == 'sigmoid':\n",
    "            self.activation = Sigmoid()\n",
    "        else:\n",
    "            self.activation = ReLU()  # Default to ReLU activation function.\n",
    "        \n",
    "    def print_weights(self):\n",
    "        print('Weights:\\n', pd.DataFrame(self.weights))\n",
    "        \n",
    "    def print_biases(self):\n",
    "        print('Biases:\\n', pd.DataFrame(self.biases))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = self.activation.activate(np.dot(self.weights, inputs) + self.biases)\n",
    "        return self.outputs\n",
    "    \n",
    "    def backward(self, delta_l, learning_rate):\n",
    "        self.weights -= learning_rate * np.dot(self.outputs.T, delta_l)\n",
    "        return delta_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import le\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layers, loss_function='cross_entropy'):\n",
    "        self.layers = layers\n",
    "        self.output = None\n",
    "        \n",
    "        if loss_function == 'cross_entropy':\n",
    "            self.loss = CrossEntropyLoss()\n",
    "        elif loss_function == 'mse':\n",
    "            self.loss = MSELoss()\n",
    "        else:\n",
    "            self.loss = CrossEntropyLoss()  # Default to cross entropy loss. \n",
    "    \n",
    "    def train(self, X_train, y_train, number_epochs, learning_rate=0.1):\n",
    "        for epoch in range(number_epochs):\n",
    "            error = 0\n",
    "            \n",
    "            for x, y in zip(X_train, y_train):\n",
    "                # Process the forward pass. This goes through every layer.\n",
    "                x = x.reshape(21, 1)\n",
    "                self.predict(x)  # Create a matrix for dot product in forward()\n",
    "                \n",
    "                # Calculate the error after the forward pass. \n",
    "                error += self.loss.calculate_loss(y, self.output)\n",
    "                # print('Prediction', self.output)\n",
    "                \n",
    "                # Error of the output layer, delta^L\n",
    "                dC_da = self.loss.derivative(y, self.output)\n",
    "                \n",
    "                da_dz = self.layers[1].activation.derivative(self.layers[1].outputs)\n",
    "                \n",
    "                delta_L = np.multiply(dC_da, da_dz)  # delta of the final layer. \n",
    "                self.layers[-1].weights = self.layers[-1].weights - learning_rate * np.dot(self.layers[-1].outputs.T, delta_L)\n",
    "\n",
    "                for layer in reversed(self.layers[:-1]):\n",
    "                    layer.backward(delta_L, learning_rate)\n",
    "                \n",
    "                da_dz_l0 = self.layers[0].activation.derivative(self.layers[0].outputs)\n",
    "                delta_l0 = np.multiply(np.dot(self.layers[1].weights.T, delta_L), da_dz_l0)\n",
    "                \n",
    "                self.layers[0].weights = self.layers[0].weights - learning_rate * np.dot(self.layers[0].outputs.T, delta_l0)\n",
    "                \n",
    "                   \n",
    "    def predict(self, x):\n",
    "        outputs = x\n",
    "        for layer in self.layers:\n",
    "            outputs = layer.forward(outputs)\n",
    "        self.output = outputs\n",
    "        return outputs\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For backpropagation, the formula for the very last layer is: \n",
    "![last-layer-backprop](https://miro.medium.com/max/828/1*zRDMl-GxVO7qENH5dNrZ-g.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # To keep results consistent.\n",
    "\n",
    "number_inputs = X.shape[1]\n",
    "epochs = 1\n",
    "\n",
    "layers = [\n",
    "    Dense(number_inputs, 2, activation_function='relu', name='Layer 1'),\n",
    "    Dense(2, 1, activation_function='sigmoid', name='Layer 2')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_layer_outputs(network):\n",
    "    for layer in network.layers:\n",
    "        print(layer.name, layer.outputs.shape, ':\\n', layer.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 21)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_practice = X[0:1]\n",
    "X_practice.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10981139 -0.2952668 ]]\n",
      "[[-0.104131   -0.28958641]]\n",
      "[[-0.09839394 -0.28384935]]\n",
      "[[-0.09251884 -0.27797426]]\n",
      "[[-0.08673864 -0.27219406]]\n",
      "[[-0.08086355 -0.26631896]]\n",
      "[[-0.07501285 -0.26046827]]\n",
      "[[-0.06913776 -0.25459317]]\n",
      "[[-0.06326267 -0.24871808]]\n",
      "[[-0.05738757 -0.24284299]]\n",
      "[[-0.05151248 -0.23696789]]\n",
      "[[-0.04563739 -0.2310928 ]]\n",
      "[[-0.0397623  -0.22521771]]\n",
      "[[-0.0338872  -0.21934262]]\n",
      "[[-0.02801211 -0.21346752]]\n",
      "[[-0.02213702 -0.20759243]]\n",
      "[[-0.01626192 -0.20171734]]\n",
      "[[-0.01038683 -0.19584225]]\n",
      "[[-0.00451174 -0.18996715]]\n",
      "[[-0.01038683 -0.19584225]]\n",
      "[[-0.01626192 -0.20171734]]\n",
      "[[-0.02213702 -0.20759243]]\n",
      "[[-0.01626192 -0.20171734]]\n",
      "[[-0.01038683 -0.19584225]]\n",
      "[[-0.00451174 -0.18996715]]\n",
      "[[ 0.00135486 -0.18410056]]\n",
      "[[ 0.00722995 -0.17822546]]\n",
      "[[ 0.01310504 -0.17235037]]\n",
      "[[ 0.01898013 -0.16647528]]\n",
      "[[ 0.02488098 -0.16057444]]\n",
      "[[ 0.03075607 -0.15469934]]\n",
      "[[ 0.03663116 -0.14882425]]\n",
      "[[ 0.04250625 -0.14294916]]\n",
      "[[ 0.04838135 -0.13707407]]\n",
      "[[ 0.05425644 -0.13119897]]\n",
      "[[ 0.06013153 -0.12532388]]\n",
      "[[ 0.06600663 -0.11944879]]\n",
      "[[ 0.06013153 -0.12532388]]\n",
      "[[ 0.06600663 -0.11944879]]\n",
      "[[ 0.07188172 -0.11357369]]\n",
      "[[ 0.07775681 -0.1076986 ]]\n",
      "[[ 0.0836319  -0.10182351]]\n",
      "[[ 0.089507   -0.09594842]]\n",
      "[[ 0.09538209 -0.09007332]]\n",
      "[[ 0.10125718 -0.08419823]]\n",
      "[[ 0.10713228 -0.07832314]]\n",
      "[[ 0.10125718 -0.08419823]]\n",
      "[[ 0.10713228 -0.07832314]]\n",
      "[[ 0.10125718 -0.08419823]]\n",
      "[[ 0.09538209 -0.09007332]]\n",
      "[[ 0.089507   -0.09594842]]\n",
      "[[ 0.0836319  -0.10182351]]\n",
      "[[ 0.07775681 -0.1076986 ]]\n",
      "[[ 0.08383312 -0.1016223 ]]\n",
      "[[ 0.08970821 -0.0957472 ]]\n",
      "[[ 0.08383312 -0.1016223 ]]\n",
      "[[ 0.08970821 -0.0957472 ]]\n",
      "[[ 0.0955833  -0.08987211]]\n",
      "[[ 0.08970821 -0.0957472 ]]\n",
      "[[ 0.08383312 -0.1016223 ]]\n",
      "[[ 0.07795802 -0.10749739]]\n",
      "[[ 0.07208293 -0.11337248]]\n",
      "[[ 0.07795802 -0.10749739]]\n",
      "[[ 0.07208293 -0.11337248]]\n",
      "[[ 0.07795802 -0.10749739]]\n",
      "[[ 0.0838533  -0.10160211]]\n",
      "[[ 0.07797821 -0.1074772 ]]\n",
      "[[ 0.07210312 -0.11335229]]\n",
      "[[ 0.06622803 -0.11922739]]\n",
      "[[ 0.06035293 -0.12510248]]\n",
      "[[ 0.06622803 -0.11922739]]\n",
      "[[ 0.06035293 -0.12510248]]\n",
      "[[ 0.06622803 -0.11922739]]\n",
      "[[ 0.07210312 -0.11335229]]\n",
      "[[ 0.06622803 -0.11922739]]\n",
      "[[ 0.07210312 -0.11335229]]\n",
      "[[ 0.06624145 -0.11921396]]\n",
      "[[ 0.07211655 -0.11333887]]\n",
      "[[ 0.07799164 -0.10746377]]\n",
      "[[ 0.07211655 -0.11333887]]\n",
      "[[ 0.06624145 -0.11921396]]\n",
      "[[ 0.06036636 -0.12508905]]\n",
      "[[ 0.06641232 -0.11904309]]\n",
      "[[ 0.07260391 -0.1128515 ]]\n",
      "[[ 0.06672882 -0.11872659]]\n",
      "[[ 0.07261394 -0.11284147]]\n",
      "[[ 0.07848904 -0.10696638]]\n",
      "[[ 0.08436413 -0.10109128]]\n",
      "[[ 0.07848904 -0.10696638]]\n",
      "[[ 0.07261576 -0.11283966]]\n",
      "[[ 0.06674066 -0.11871475]]\n",
      "[[ 0.07261576 -0.11283966]]\n",
      "[[ 0.06674066 -0.11871475]]\n",
      "[[ 0.06086557 -0.12458984]]\n",
      "[[ 0.06674066 -0.11871475]]\n",
      "[[ 0.07261576 -0.11283966]]\n",
      "[[ 0.06674066 -0.11871475]]\n",
      "[[ 0.06086557 -0.12458984]]\n",
      "[[ 0.05499252 -0.13046289]]\n",
      "[[ 0.06086762 -0.1245878 ]]\n",
      "[[ 0.06674271 -0.1187127 ]]\n",
      "[[ 0.06086969 -0.12458572]]\n",
      "[[ 0.0549946  -0.13046082]]\n",
      "[[ 0.0491195  -0.13633591]]\n",
      "[[ 0.04324441 -0.142211  ]]\n",
      "[[ 0.0491195  -0.13633591]]\n",
      "[[ 0.04324441 -0.142211  ]]\n",
      "[[ 0.03736932 -0.1480861 ]]\n",
      "[[ 0.04324441 -0.142211  ]]\n",
      "[[ 0.03736932 -0.1480861 ]]\n",
      "[[ 0.03149423 -0.15396119]]\n",
      "[[ 0.02561913 -0.15983628]]\n",
      "[[ 0.01974404 -0.16571137]]\n",
      "[[ 0.01386895 -0.17158647]]\n",
      "[[ 0.00799385 -0.17746156]]\n",
      "[[ 0.00211876 -0.18333665]]\n",
      "[[-0.00375633 -0.18921174]]\n",
      "[[ 0.00211876 -0.18333665]]\n",
      "[[ 0.00799385 -0.17746156]]\n",
      "[[ 0.01386895 -0.17158647]]\n",
      "[[ 0.00799385 -0.17746156]]\n",
      "[[ 0.01387153 -0.17158389]]\n",
      "[[ 0.01980072 -0.1656547 ]]\n",
      "[[ 0.0139275  -0.17152791]]\n",
      "[[ 0.00805241 -0.177403  ]]\n",
      "[[ 0.00217732 -0.1832781 ]]\n",
      "[[ 0.00805241 -0.177403  ]]\n",
      "[[ 0.0139275  -0.17152791]]\n",
      "[[ 0.00805254 -0.17740288]]\n",
      "[[ 0.0139278  -0.17152761]]\n",
      "[[ 0.00805289 -0.17740252]]\n",
      "[[ 0.01392798 -0.17152743]]\n",
      "[[ 0.01980308 -0.16565234]]\n",
      "[[ 0.01392954 -0.17152587]]\n",
      "[[ 0.01980464 -0.16565078]]\n",
      "[[ 0.02567973 -0.15977568]]\n",
      "[[ 0.01980464 -0.16565078]]\n",
      "[[ 0.01392954 -0.17152587]]\n",
      "[[ 0.01980464 -0.16565078]]\n",
      "[[ 0.01392954 -0.17152587]]\n",
      "[[ 0.00805479 -0.17740063]]\n",
      "[[ 0.01392988 -0.17152553]]\n",
      "[[ 0.00805479 -0.17740063]]\n",
      "[[ 0.00217969 -0.18327572]]\n",
      "[[-0.0036954  -0.18915081]]\n",
      "[[-0.00957053 -0.19502594]]\n",
      "[[-0.00369543 -0.18915085]]\n",
      "[[-0.00957053 -0.19502594]]\n",
      "[[-0.01544569 -0.20090111]]\n",
      "[[-0.02132078 -0.2067762 ]]\n",
      "[[-0.02719588 -0.21265129]]\n",
      "[[-0.03307097 -0.21852638]]\n",
      "[[-0.03894606 -0.22440148]]\n",
      "[[-0.04482116 -0.23027657]]\n",
      "[[-0.05069625 -0.23615166]]\n",
      "[[-0.05657134 -0.24202675]]\n",
      "[[-0.05069625 -0.23615166]]\n",
      "[[-0.05657134 -0.24202675]]\n",
      "[[-0.06244745 -0.24790286]]\n",
      "[[-0.06832254 -0.25377795]]\n",
      "[[-0.07419763 -0.25965304]]\n",
      "[[-0.06832254 -0.25377795]]\n",
      "[[-0.06244745 -0.24790286]]\n",
      "[[-0.06832254 -0.25377795]]\n",
      "[[-0.06256586 -0.24802128]]\n",
      "[[-0.06844095 -0.25389637]]\n",
      "[[-0.07431797 -0.25977338]]\n",
      "[[-0.06844288 -0.25389829]]\n",
      "[[-0.06256779 -0.2480232 ]]\n",
      "[[-0.06844399 -0.25389941]]\n",
      "[[-0.07431954 -0.25977495]]\n",
      "[[-0.06844445 -0.25389986]]\n",
      "[[-0.06265076 -0.24810617]]\n",
      "[[-0.06852585 -0.25398127]]\n",
      "[[-0.07440095 -0.25985636]]\n",
      "[[-0.08027709 -0.2657325 ]]\n",
      "[[-0.08615218 -0.27160759]]\n",
      "[[-0.08027709 -0.2657325 ]]\n",
      "[[-0.08615218 -0.27160759]]\n",
      "[[-0.09202727 -0.27748269]]\n",
      "[[-0.0867005  -0.27215592]]\n",
      "[[-0.08091294 -0.26636836]]\n",
      "[[-0.07503785 -0.26049326]]\n",
      "[[-0.08091294 -0.26636836]]\n",
      "[[-0.07503785 -0.26049326]]\n",
      "[[-0.08091294 -0.26636836]]\n",
      "[[-0.07508821 -0.26054362]]\n",
      "[[-0.0809633  -0.26641872]]\n",
      "[[-0.0868384  -0.27229381]]\n",
      "[[-0.09271349 -0.2781689 ]]\n",
      "[[-0.0868384  -0.27229381]]\n",
      "[[-0.09271349 -0.2781689 ]]\n",
      "[[-0.09859202 -0.28404743]]\n",
      "[[-0.09271693 -0.27817234]]\n",
      "[[-0.08684183 -0.27229725]]\n",
      "[[-0.09271693 -0.27817234]]\n",
      "[[-0.08684183 -0.27229725]]\n",
      "[[-0.08096674 -0.26642215]]\n",
      "[[-0.07509165 -0.26054706]]\n",
      "[[-0.06921656 -0.25467197]]\n",
      "[[-0.07509165 -0.26054706]]\n",
      "[[-0.06921915 -0.25467457]]\n",
      "[[-0.06344234 -0.24889776]]\n",
      "[[-0.05756725 -0.24302266]]\n",
      "[[-0.06344234 -0.24889776]]\n",
      "[[-0.05756725 -0.24302266]]\n",
      "[[-0.06344234 -0.24889776]]\n",
      "[[-0.05756725 -0.24302266]]\n",
      "[[-0.06344234 -0.24889776]]\n",
      "[[-0.06931743 -0.25477285]]\n",
      "[[-0.06344234 -0.24889776]]\n",
      "[[-0.06931743 -0.25477285]]\n",
      "[[-0.06386818 -0.2493236 ]]\n",
      "[[-0.05799309 -0.2434485 ]]\n",
      "[[-0.052118   -0.23757341]]\n",
      "[[-0.0462429  -0.23169832]]\n",
      "[[-0.052118   -0.23757341]]\n",
      "[[-0.05799309 -0.2434485 ]]\n",
      "[[-0.052118   -0.23757341]]\n",
      "[[-0.0462429  -0.23169832]]\n",
      "[[-0.052118   -0.23757341]]\n",
      "[[-0.05799309 -0.2434485 ]]\n",
      "[[-0.06386818 -0.2493236 ]]\n",
      "[[-0.05799309 -0.2434485 ]]\n",
      "[[-0.06386818 -0.2493236 ]]\n",
      "[[-0.06974328 -0.25519869]]\n",
      "[[-0.07561837 -0.26107378]]\n",
      "[[-0.08149346 -0.26694887]]\n",
      "[[-0.08736855 -0.27282397]]\n",
      "[[-0.08149346 -0.26694887]]\n",
      "[[-0.07581351 -0.26126892]]\n",
      "[[-0.0816886  -0.26714401]]\n",
      "[[-0.08756369 -0.2730191 ]]\n",
      "[[-0.0816886  -0.26714401]]\n",
      "[[-0.08756369 -0.2730191 ]]\n",
      "[[-0.09343878 -0.2788942 ]]\n",
      "[[-0.08756369 -0.2730191 ]]\n",
      "[[-0.0816886  -0.26714401]]\n",
      "[[-0.08756369 -0.2730191 ]]\n",
      "[[-0.0816886  -0.26714401]]\n",
      "[[-0.08756369 -0.2730191 ]]\n",
      "[[-0.09343888 -0.27889429]]\n",
      "[[-0.09931397 -0.28476938]]\n",
      "[[-0.10518906 -0.29064448]]\n",
      "[[-0.09931397 -0.28476938]]\n",
      "[[-0.10518906 -0.29064448]]\n",
      "[[-0.11106416 -0.29651957]]\n",
      "[[-0.11693925 -0.30239466]]\n",
      "[[-0.12281434 -0.30826975]]\n",
      "[[-0.12868943 -0.31414485]]\n",
      "[[-0.12281434 -0.30826975]]\n",
      "[[-0.12868943 -0.31414485]]\n",
      "[[-0.12281434 -0.30826975]]\n",
      "[[-0.11710513 -0.30256055]]\n",
      "[[-0.11123004 -0.29668546]]\n",
      "[[-0.10535495 -0.29081036]]\n",
      "[[-0.09947986 -0.28493527]]\n",
      "[[-0.09371472 -0.27917013]]\n",
      "[[-0.08783962 -0.27329504]]\n",
      "[[-0.08196453 -0.26741994]]\n",
      "[[-0.07609049 -0.2615459 ]]\n",
      "[[-0.07021539 -0.25567081]]\n",
      "[[-0.0643403  -0.24979571]]\n",
      "[[-0.05846521 -0.24392062]]\n",
      "[[-0.05259012 -0.23804553]]\n",
      "[[-0.04671502 -0.23217044]]\n",
      "[[-0.05259012 -0.23804553]]\n",
      "[[-0.05846521 -0.24392062]]\n",
      "[[-0.0643403  -0.24979571]]\n",
      "[[-0.07021539 -0.25567081]]\n",
      "[[-0.07609049 -0.2615459 ]]\n",
      "[[-0.08196758 -0.26742299]]\n",
      "[[-0.07609249 -0.2615479 ]]\n",
      "[[-0.08196758 -0.26742299]]\n",
      "[[-0.07609249 -0.2615479 ]]\n",
      "[[-0.08196758 -0.26742299]]\n",
      "[[-0.08784267 -0.27329809]]\n",
      "[[-0.08197891 -0.26743432]]\n",
      "[[-0.087854   -0.27330941]]\n",
      "[[-0.09372928 -0.27918469]]\n",
      "[[-0.08785419 -0.2733096 ]]\n",
      "[[-0.09372928 -0.27918469]]\n",
      "[[-0.08788459 -0.27334   ]]\n",
      "[[-0.0820095  -0.26746491]]\n",
      "[[-0.08788459 -0.27334   ]]\n",
      "[[-0.09375968 -0.2792151 ]]\n",
      "[[-0.09963478 -0.28509019]]\n",
      "[[-0.10550987 -0.29096528]]\n",
      "[[-0.11138496 -0.29684038]]\n",
      "[[-0.11726006 -0.30271547]]\n",
      "[[-0.12313515 -0.30859056]]\n",
      "[[-0.12901097 -0.31446638]]\n",
      "[[-0.13488606 -0.32034147]]\n",
      "[[-0.14076115 -0.32621656]]\n",
      "[[-0.14663639 -0.3320918 ]]\n",
      "[[-0.15251639 -0.33797181]]\n",
      "[[-0.15839149 -0.3438469 ]]\n",
      "[[-0.15251639 -0.33797181]]\n",
      "[[-0.15839149 -0.3438469 ]]\n",
      "[[-0.16426658 -0.34972199]]\n",
      "[[-0.15839149 -0.3438469 ]]\n",
      "[[-0.16426658 -0.34972199]]\n",
      "[[-0.15854211 -0.34399752]]\n",
      "[[-0.1644172  -0.34987261]]\n",
      "[[-0.17029229 -0.3557477 ]]\n",
      "[[-0.17616738 -0.3616228 ]]\n",
      "[[-0.18204248 -0.36749789]]\n",
      "[[-0.18791757 -0.37337298]]\n",
      "[[-0.19379266 -0.37924808]]\n",
      "[[-0.19966775 -0.38512317]]\n",
      "[[-0.20554285 -0.39099826]]\n",
      "[[-0.21141794 -0.39687335]]\n",
      "[[-0.21729303 -0.40274845]]\n",
      "[[-0.22317094 -0.40862635]]\n",
      "[[-0.22904603 -0.41450144]]\n",
      "[[-0.23492112 -0.42037653]]\n",
      "[[-0.24079621 -0.42625163]]\n",
      "[[-0.23501475 -0.42047016]]\n",
      "[[-0.24088984 -0.42634526]]\n",
      "[[-0.24676494 -0.43222035]]\n",
      "[[-0.25264003 -0.43809544]]\n",
      "[[-0.24699751 -0.43245293]]\n",
      "[[-0.25287766 -0.43833307]]\n",
      "[[-0.24772031 -0.43317572]]\n",
      "[[-0.2535954  -0.43905082]]\n",
      "[[-0.2594705  -0.44492591]]\n",
      "[[-0.26534846 -0.45080388]]\n",
      "[[-0.27122356 -0.45667897]]\n",
      "[[-0.26534846 -0.45080388]]\n",
      "[[-0.25947337 -0.44492878]]\n",
      "[[-0.25359828 -0.43905369]]\n",
      "[[-0.25947337 -0.44492878]]\n",
      "[[-0.26534846 -0.45080388]]\n",
      "[[-0.27122356 -0.45667897]]\n",
      "[[-0.27709865 -0.46255406]]\n",
      "[[-0.27122356 -0.45667897]]\n",
      "[[-0.27710506 -0.46256047]]\n",
      "[[-0.27122996 -0.45668538]]\n",
      "[[-0.27710506 -0.46256047]]\n",
      "[[-0.27153602 -0.45699144]]\n",
      "[[-0.27741112 -0.46286653]]\n",
      "[[-0.28328621 -0.46874162]]\n",
      "[[-0.28916267 -0.47461808]]\n",
      "[[-0.28332861 -0.46878402]]\n",
      "[[-0.2892037  -0.47465912]]\n",
      "[[-0.2950788  -0.48053421]]\n",
      "[[-0.30095389 -0.4864093 ]]\n",
      "[[-0.30682898 -0.4922844 ]]\n",
      "[[-0.31270408 -0.49815949]]\n",
      "[[-0.31857917 -0.50403458]]\n",
      "[[-0.32445426 -0.50990967]]\n",
      "[[-0.31857917 -0.50403458]]\n",
      "[[-0.31369993 -0.49915534]]\n",
      "[[-0.30782483 -0.49328025]]\n",
      "[[-0.31369993 -0.49915534]]\n",
      "[[-0.31957502 -0.50503043]]\n",
      "[[-0.32545011 -0.51090553]]\n",
      "[[-0.33132521 -0.51678062]]\n",
      "[[-0.3372003  -0.52265571]]\n",
      "[[-0.34307539 -0.5285308 ]]\n",
      "[[-0.34895048 -0.5344059 ]]\n",
      "[[-0.35482558 -0.54028099]]\n",
      "[[-0.36070067 -0.54615608]]\n",
      "[[-0.36657576 -0.55203118]]\n",
      "[[-0.37245086 -0.55790627]]\n",
      "[[-0.36669759 -0.552153  ]]\n",
      "[[-0.3608225  -0.54627791]]\n",
      "[[-0.36669759 -0.552153  ]]\n",
      "[[-0.3608225  -0.54627791]]\n",
      "[[-0.35577006 -0.54122548]]\n",
      "[[-0.34989497 -0.53535038]]\n",
      "[[-0.35577006 -0.54122548]]\n",
      "[[-0.35136572 -0.53682114]]\n",
      "[[-0.34571867 -0.53117408]]\n",
      "[[-0.35159376 -0.53704917]]\n",
      "[[-0.35748591 -0.54294132]]\n",
      "[[-0.363361   -0.54881642]]\n",
      "[[-0.3692361  -0.55469151]]\n",
      "[[-0.37511119 -0.5605666 ]]\n",
      "[[-0.3692361  -0.55469151]]\n",
      "[[-0.3751247  -0.56058011]]\n",
      "[[-0.38099979 -0.5664552 ]]\n",
      "[[-0.38687488 -0.5723303 ]]\n",
      "[[-0.39274998 -0.57820539]]\n",
      "[[-0.39862507 -0.58408048]]\n",
      "[[-0.39274998 -0.57820539]]\n",
      "[[-0.39862507 -0.58408048]]\n",
      "[[-0.40450016 -0.58995558]]\n",
      "[[-0.41037526 -0.59583067]]\n",
      "[[-0.40450016 -0.58995558]]\n",
      "[[-0.41038399 -0.5958394 ]]\n",
      "[[-0.41625908 -0.6017145 ]]\n",
      "[[-0.41038399 -0.5958394 ]]\n",
      "[[-0.4050023  -0.59045771]]\n",
      "[[-0.41087739 -0.5963328 ]]\n",
      "[[-0.41675248 -0.6022079 ]]\n",
      "[[-0.42262758 -0.60808299]]\n",
      "[[-0.42850267 -0.61395808]]\n",
      "[[-0.43437776 -0.61983318]]\n",
      "[[-0.44025285 -0.62570827]]\n",
      "[[-0.43437776 -0.61983318]]\n",
      "[[-0.44025285 -0.62570827]]\n",
      "[[-0.44612795 -0.63158336]]\n",
      "[[-0.45200304 -0.63745845]]\n",
      "[[-0.45787813 -0.64333355]]\n",
      "[[-0.46375323 -0.64920864]]\n",
      "[[-0.46964243 -0.65509784]]\n",
      "[[-0.47551752 -0.66097293]]\n",
      "[[-0.46983355 -0.65528897]]\n",
      "[[-0.47570865 -0.66116406]]\n",
      "[[-0.48158374 -0.66703915]]\n",
      "[[-0.48745883 -0.67291424]]\n",
      "[[-0.49333392 -0.67878934]]\n",
      "[[-0.49920902 -0.68466443]]\n",
      "[[-0.49333392 -0.67878934]]\n",
      "[[-0.49920902 -0.68466443]]\n",
      "[[-0.50508411 -0.69053952]]\n",
      "[[-0.49920902 -0.68466443]]\n",
      "[[-0.50509651 -0.69055193]]\n",
      "[[-0.51097161 -0.69642702]]\n",
      "[[-0.5168467  -0.70230211]]\n",
      "[[-0.52272179 -0.7081772 ]]\n",
      "[[-0.52859688 -0.7140523 ]]\n",
      "[[-0.53447198 -0.71992739]]\n",
      "[[-0.54034707 -0.72580248]]\n",
      "[[-0.54622216 -0.73167758]]\n",
      "[[-0.55209726 -0.73755267]]\n",
      "[[-0.55797235 -0.74342776]]\n",
      "[[-0.56384744 -0.74930285]]\n",
      "[[-0.56972253 -0.75517795]]\n",
      "[[-0.56384744 -0.74930285]]\n",
      "[[-0.56972253 -0.75517795]]\n",
      "[[-0.56463887 -0.75009428]]\n",
      "[[-0.55876377 -0.74421919]]\n",
      "[[-0.56463887 -0.75009428]]\n",
      "[[-0.55876377 -0.74421919]]\n",
      "[[-0.56463887 -0.75009428]]\n",
      "[[-0.57051396 -0.75596937]]\n",
      "[[-0.57638905 -0.76184447]]\n",
      "[[-0.58226414 -0.76771956]]\n",
      "[[-0.58813924 -0.77359465]]\n",
      "[[-0.58226414 -0.76771956]]\n",
      "[[-0.58813924 -0.77359465]]\n",
      "[[-0.59401433 -0.77946974]]\n",
      "[[-0.58813924 -0.77359465]]\n",
      "[[-0.59401433 -0.77946974]]\n",
      "[[-0.58813924 -0.77359465]]\n",
      "[[-0.59401433 -0.77946974]]\n",
      "[[-0.59988942 -0.78534484]]\n",
      "[[-0.59465647 -0.78011188]]\n",
      "[[-0.60053156 -0.78598697]]\n",
      "[[-0.59465647 -0.78011188]]\n",
      "[[-0.60053156 -0.78598697]]\n",
      "[[-0.60642389 -0.79187931]]\n",
      "[[-0.61229899 -0.7977544 ]]\n",
      "[[-0.61817408 -0.80362949]]\n",
      "[[-0.62404917 -0.80950459]]\n",
      "[[-0.62992426 -0.81537968]]\n",
      "[[-0.63579936 -0.82125477]]\n",
      "[[-0.64167445 -0.82712986]]\n",
      "[[-0.63579936 -0.82125477]]\n",
      "[[-0.62992426 -0.81537968]]\n",
      "[[-0.63579936 -0.82125477]]\n",
      "[[-0.64167445 -0.82712986]]\n",
      "[[-0.64754954 -0.83300496]]\n",
      "[[-0.65342464 -0.83888005]]\n",
      "[[-0.65929973 -0.84475514]]\n",
      "[[-0.66517482 -0.85063023]]\n",
      "[[-0.65929973 -0.84475514]]\n",
      "[[-0.66517482 -0.85063023]]\n",
      "[[-0.67104991 -0.85650533]]\n",
      "[[-0.67692501 -0.86238042]]\n",
      "[[-0.6828001  -0.86825551]]\n",
      "[[-0.68867519 -0.87413061]]\n",
      "[[-0.69455029 -0.8800057 ]]\n",
      "[[-0.70042538 -0.88588079]]\n",
      "[[-0.70630047 -0.89175588]]\n",
      "[[-0.71217556 -0.89763098]]\n",
      "[[-0.71805066 -0.90350607]]\n",
      "[[-0.71217556 -0.89763098]]\n",
      "[[-0.71805066 -0.90350607]]\n",
      "[[-0.72392575 -0.90938116]]\n",
      "[[-0.729796   -0.91525142]]\n",
      "[[-0.7356711  -0.92112651]]\n",
      "[[-0.74156564 -0.92702106]]\n",
      "[[-0.74744074 -0.93289615]]\n",
      "[[-0.75331583 -0.93877124]]\n",
      "[[-0.74761813 -0.93307354]]\n",
      "[[-0.75349322 -0.93894863]]\n",
      "[[-0.74761813 -0.93307354]]\n",
      "[[-0.75349322 -0.93894863]]\n",
      "[[-0.75938909 -0.9448445 ]]\n",
      "[[-0.753514   -0.93896941]]\n",
      "[[-0.75938909 -0.9448445 ]]\n",
      "[[-0.76526418 -0.9507196 ]]\n",
      "[[-0.77113927 -0.95659469]]\n",
      "[[-0.77701437 -0.96246978]]\n",
      "[[-0.78288946 -0.96834487]]\n",
      "[[-0.77703308 -0.96248849]]\n",
      "[[-0.77160476 -0.95706017]]\n",
      "[[-0.77749861 -0.96295402]]\n",
      "[[-0.77162351 -0.95707893]]\n",
      "[[-0.77749861 -0.96295402]]\n",
      "[[-0.77162351 -0.95707893]]\n",
      "[[-0.77749861 -0.96295402]]\n",
      "[[-0.78339015 -0.96884557]]\n",
      "[[-0.78926524 -0.97472066]]\n",
      "[[-0.79516114 -0.98061655]]\n",
      "[[-0.80105702 -0.98651243]]\n",
      "[[-0.79518193 -0.98063734]]\n",
      "[[-0.80105702 -0.98651243]]\n",
      "[[-0.80693211 -0.99238753]]\n",
      "[[-0.80105702 -0.98651243]]\n",
      "[[-0.80693211 -0.99238753]]\n",
      "[[-0.80105702 -0.98651243]]\n",
      "[[-0.80693211 -0.99238753]]\n",
      "[[-0.80146092 -0.98691633]]\n",
      "[[-0.79589361 -0.98134903]]\n",
      "[[-0.80176871 -0.98722412]]\n",
      "[[-0.8076438  -0.99309921]]\n",
      "[[-0.81351889 -0.99897431]]\n",
      "[[-0.81025099 -0.9957064 ]]\n",
      "[[-0.81612608 -1.0015815 ]]\n",
      "[[-0.82200118 -1.00745659]]\n",
      "[[-0.82787627 -1.01333168]]\n",
      "[[-0.83375136 -1.01920678]]\n",
      "[[-0.83962645 -1.02508187]]\n",
      "[[-0.84551936 -1.03097477]]\n",
      "[[-0.85139445 -1.03684987]]\n",
      "[[-0.85726955 -1.04272496]]\n",
      "[[-0.86314464 -1.04860005]]\n",
      "[[-0.86901973 -1.05447515]]\n",
      "[[-0.87489933 -1.06035475]]\n",
      "[[-0.86931054 -1.05476596]]\n",
      "[[-0.87518564 -1.06064105]]\n",
      "[[-0.87147978 -1.0569352 ]]\n",
      "[[-0.86560469 -1.0510601 ]]\n",
      "[[-0.87147978 -1.0569352 ]]\n",
      "[[-0.87735488 -1.06281029]]\n",
      "[[-0.88322997 -1.06868538]]\n",
      "[[-0.88910506 -1.07456047]]\n",
      "[[-0.89498015 -1.08043557]]\n",
      "[[-0.90085525 -1.08631066]]\n",
      "[[-0.90673034 -1.09218575]]\n",
      "[[-0.91260543 -1.09806085]]\n",
      "[[-0.91848053 -1.10393594]]\n",
      "[[-0.92435562 -1.10981103]]\n",
      "[[-0.93023071 -1.11568612]]\n",
      "[[-0.9361058  -1.12156122]]\n",
      "[[-0.9419809  -1.12743631]]\n",
      "[[-0.94785599 -1.1333114 ]]\n",
      "[[-0.95373108 -1.1391865 ]]\n",
      "[[-0.95960617 -1.14506159]]\n",
      "[[-0.96548127 -1.15093668]]\n",
      "[[-0.97135636 -1.15681177]]\n",
      "[[-0.97723145 -1.16268687]]\n",
      "[[-0.98310655 -1.16856196]]\n",
      "[[-0.98898164 -1.17443705]]\n",
      "[[-0.99485673 -1.18031214]]\n",
      "[[-1.00073182 -1.18618724]]\n",
      "[[-1.00660692 -1.19206233]]\n",
      "[[-1.01248201 -1.19793742]]\n",
      "[[-1.00660692 -1.19206233]]\n",
      "[[-1.00073182 -1.18618724]]\n",
      "[[-0.99545243 -1.18090784]]\n",
      "[[-0.98957734 -1.17503275]]\n",
      "[[-0.98370224 -1.16915766]]\n",
      "[[-0.97782715 -1.16328256]]\n",
      "[[-0.98370224 -1.16915766]]\n"
     ]
    }
   ],
   "source": [
    "network = Network(layers, loss_function='mse')\n",
    "network.train(X, y, number_epochs=epochs)\n",
    "# print_layer_outputs(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = []\n",
    "for i in X:\n",
    "    pred.append(network.predict(i.reshape(21, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame()\n",
    "res[\"predictions\"] = pred\n",
    "res[\"actual\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[\"predictions\"] = res[\"predictions\"].apply(lambda x: x[0][0])\n",
    "res[\"predictions\"] = res[\"predictions\"].apply(lambda x: 0 if x < 0.5 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 64.32337434094903\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", sum(x == y for x, y in zip(res['predictions'], res['actual'])) / len(X) * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('F21DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d80a60b5cae90d0132c97af96d84f3faaaa5679a5193fa9421d1db0d11796e0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
