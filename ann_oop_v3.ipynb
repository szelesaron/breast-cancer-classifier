{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's start over after we've understood some maths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Data Setup:\n",
    "X = np.load('data/X-data.npy')\n",
    "y = np.load('data/y-data.npy')\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstract Class Definitions: \n",
    "class Layer():\n",
    "    def __init__(self): \n",
    "        self.inputs = None  # The inputs into this layer. \n",
    "        self.outputs = None # The ouputs of this layer. \n",
    "        \n",
    "    # Forward propagation method.\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "    \n",
    "    # Backward propagation method.\n",
    "    def backward(self):\n",
    "        pass\n",
    "    \n",
    "class Activation():\n",
    "    def activate():\n",
    "        pass\n",
    "    \n",
    "    def prime():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    def activate(self, x): \n",
    "        return np.maximum(x, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense (fully connected) Layer Class:\n",
    "class Dense(Layer): \n",
    "    def __init__(self, input_size, output_size, activation_function='relu'):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.biases = np.random.randn(output_size)\n",
    "        self.outputs = None\n",
    "        \n",
    "        if activation_function == 'relu':\n",
    "            self.activation = ReLU()\n",
    "        else:\n",
    "            self.activation = ReLU()  # Default to ReLU activation function.\n",
    "        \n",
    "    def print_weights(self):\n",
    "        print('Weights:\\n', pd.DataFrame(self.weights))\n",
    "        \n",
    "    def print_biases(self):\n",
    "        print('Biases:\\n', pd.DataFrame(self.biases))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.weights = self.weights * inputs + self.biases\n",
    "        self.outputs = self.activation.activate(self.weights)\n",
    "    \n",
    "    def backward(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior to feedforward:\n",
      "Weights:\n",
      "          0         1         2         3         4         5         6   \\\n",
      "0  1.764052  0.400157  0.978738  2.240893  1.867558 -0.977278  0.950088   \n",
      "\n",
      "         7         8         9   ...        11        12        13        14  \\\n",
      "0 -0.151357 -0.103219  0.410599  ...  1.454274  0.761038  0.121675  0.443863   \n",
      "\n",
      "         15        16        17        18        19       20  \n",
      "0  0.333674  1.494079 -0.205158  0.313068 -0.854096 -2.55299  \n",
      "\n",
      "[1 rows x 21 columns]\n",
      "Biases:\n",
      "           0\n",
      "0  0.653619\n",
      "After feedforward:\n",
      "Weights:\n",
      "          0         1         2        3         4         5         6   \\\n",
      "0  1.788159  0.762691  1.256309  1.77763  0.993081  0.454637  0.984969   \n",
      "\n",
      "         7         8         9   ...       11        12        13     14  \\\n",
      "0  0.632228  0.637471  0.704714  ...  0.82724  0.715508  0.684504  0.923   \n",
      "\n",
      "         15        16        17        18        19        20  \n",
      "0  0.754913  1.460149  0.564331  0.702007  0.488803 -0.978189  \n",
      "\n",
      "[1 rows x 21 columns]\n",
      "Biases:\n",
      "           0\n",
      "0  0.653619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.78815915, 0.76269087, 1.25630909, 1.77762971, 0.99308086,\n",
       "        0.45463676, 0.98496901, 0.63222839, 0.63747138, 0.70471366,\n",
       "        0.67171908, 0.82723958, 0.7155085 , 0.68450409, 0.92299984,\n",
       "        0.75491259, 1.46014895, 0.56433078, 0.70200739, 0.48880268,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)  # To keep results consistent.\n",
    "\n",
    "number_inputs = X.shape[1]\n",
    "perceptron = Dense(number_inputs, 1)\n",
    "\n",
    "print('Prior to feedforward:')\n",
    "perceptron.print_weights()\n",
    "perceptron.print_biases()\n",
    "\n",
    "perceptron.forward(X[1])\n",
    "\n",
    "print('After feedforward:')\n",
    "perceptron.print_weights()\n",
    "perceptron.print_biases()\n",
    "perceptron.outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('F21DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d80a60b5cae90d0132c97af96d84f3faaaa5679a5193fa9421d1db0d11796e0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
