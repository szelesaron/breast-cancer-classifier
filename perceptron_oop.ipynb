{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A single Perceptron\n",
    "We are using [this](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29) data.\n",
    "This is code to implement a single perceptron in Object Oriented Programming using the data from the coursework. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Data Setup\n",
    "Exploratory Data Analysis and explanation for preprocessing can be found in perceptron.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Loading the pre-processed data. Preprocessing can be found in perceptron.ipynb.\n",
    "X = np.load(\"data/X-data.npy\")\n",
    "y = np.load(\"data/y-data.npy\")\n",
    "\n",
    "# Setting a random seed for consistency. \n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object-Oriented Approach\n",
    "#### Layers\n",
    "Each layer in an ANN is made up of a number of neurons. Each of those neurons is just a visualization to make it easier for humans to understand how a neural network works.  \n",
    "In practice, what you need for a layer on a very basic level is:\n",
    "- an array of weights\n",
    "- an array of biases\n",
    "- an array of outputs\n",
    "The length of all three arrays will be the same and depict the **number of neurons** on this specific layer.  \n",
    "  \n",
    "Then we also need an input for the layer. For the first layer, that will be our data. For the hidden layers and the output layer, that will be the output from the layer before. The input will need to be passed to the layer.  \n",
    "  \n",
    "Each layer must be able to process `activation_function(inputs * weights + bias)` and assign that to `layer.outputs`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables.\n",
    "learning_rate = 0.1\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nr_inputs, nr_neurons, activation_function):  # Constructor.\n",
    "        self.weights = np.zeros(nr_inputs)  # Create random initial weights.\n",
    "        self.biases = np.random.randn(nr_neurons)  # Create random initial biases.\n",
    "        self.activation = activation_function  # I love how you can pass functions in python. \n",
    "        \n",
    "        # Variables used for Backpropagation.\n",
    "        \n",
    "        \n",
    "        self.output = []  # To save the outputs of this layer.\n",
    "     \n",
    "    # For pretty printing.   \n",
    "    def __str__(self):\n",
    "        return f\"Weights: {self.weights},\\nBiases: {self.biases}\\nActivation: {self.activation}.\"\n",
    "        \n",
    "    def forward_propagation(self, inputs):\n",
    "        self.output.append(self.activation(np.dot(inputs, self.weights) + self.biases)[0])\n",
    "        \n",
    "    def back_propagation(self, y, y_hat):\n",
    "        self.weights += learning_rate * (y - y_hat) * y_hat\n",
    "        self.biases += learning_rate * (y - y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions\n",
    "Then we need a collection of different activation functions, as each of our layers will have a specific activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    def __init__(self, step_threshold=0.5): \n",
    "        self.step_threshold = step_threshold  # Use a default threshold of 0.5.\n",
    "        \n",
    "    def relu(x):\n",
    "        return np.maximum(x, 0.0)\n",
    "    \n",
    "    def step(self, x):\n",
    "        for row, element in enumerate(x):\n",
    "            x[row] = 1.0 if element > self.step_threshold else 0.0\n",
    "        return x\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_function = ActivationFunctions().step\n",
    "perceptron = Layer(X.shape[1], 1, step_function)\n",
    "\n",
    "for index, x in enumerate(X):\n",
    "    perceptron.forward_propagation(x)\n",
    "    perceptron.back_propagation(y[index], perceptron.output[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  86.29173989455184\n"
     ]
    }
   ],
   "source": [
    "res = pd.DataFrame()\n",
    "res['Predictions'] = perceptron.output\n",
    "res['Predictions'] = res['Predictions'].apply(lambda x: 0 if x < 0.5 else 1)\n",
    "res['Expectation'] = y\n",
    "\n",
    "print(\"Accuracy: \", res.loc[res['Predictions']==res['Expectation']].shape[0] / res.shape[0] * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('F21DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d80a60b5cae90d0132c97af96d84f3faaaa5679a5193fa9421d1db0d11796e0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
